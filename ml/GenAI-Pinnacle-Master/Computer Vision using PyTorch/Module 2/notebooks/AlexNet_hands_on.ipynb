{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ddfc72",
   "metadata": {},
   "source": [
    "# Building an Image Classification Model Using AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af416ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T04:24:36.451957Z",
     "start_time": "2025-03-30T04:24:34.843154Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pvishnoi/PycharmProjects/data-etl-ml/venv/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ad2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the training and validation sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)), #Alexnet expects an input size of 227 x 227 x 3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a875f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "train_data_path = '/Users/pvishnoi/PycharmProjects/data-etl-ml/ml/GenAI-Pinnacle-Master/Computer Vision using PyTorch/Module 2/Human Action Recognition//train'\n",
    "test_data_path = '/Users/pvishnoi/PycharmProjects/data-etl-ml/ml/GenAI-Pinnacle-Master/Computer Vision using PyTorch/Module 2/Human Action Recognition//test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3993fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the transforms to the train and test data\n",
    "train_data = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "test_data = datasets.ImageFolder(root=test_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d2298",
   "metadata": {},
   "source": [
    "##### Run all the cells above till here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea661b",
   "metadata": {},
   "source": [
    "### AlexNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51005e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the feature map after the last pooling layer\n",
    "        self.fc_input_dim = 256 * 6 * 6  # This is for input size 224x224\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.fc_input_dim, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ae626",
   "metadata": {},
   "source": [
    "Changes in the architecture are as follows:\n",
    "\n",
    "\n",
    "1)__Increased Depth of Convolutional Layers:__ The AlexNet model uses more layers and deeper configurations. Starting with fewer feature channels, it progressively increases the depth across its layers. This contrasts sharply with LeNet, which uses a much simpler and shallower layer structure.\n",
    "\n",
    "\n",
    "2)__Use of Batch Normalization:__ Each convolutional layer in AlexNet is followed by batch normalization, which helps in stabilizing the learning process by normalizing the input layer by adjusting and scaling activations. LeNet does not include batch normalization, which makes AlexNet better suited for training on larger datasets and more complex models.\n",
    "\n",
    "\n",
    "3)__Advanced Activation Functions:__ AlexNet incorporates the ReLU (Rectified Linear Unit) activation function throughout its architecture, which helps to solve the vanishing gradient problem common in networks using sigmoid or tanh functions. LeNet traditionally uses sigmoid or tanh, which are less effective at preserving gradients during deep learning training.\n",
    "\n",
    "\n",
    "4)__Increased and Varied Pooling Layers:__ AlexNet includes multiple max pooling layers that perform downsampling operation more frequently compared to LeNet. This reduces the spatial size of the representation, decreases the number of parameters and computation in the network, and hence, also controls overfitting.\n",
    "\n",
    "\n",
    "5)__Integration of Dropout:__ In the classifier section of AlexNet, dropout layers are introduced. Dropout is a form of regularization that helps in preventing overfitting by randomly setting a fraction of input units to 0 at each update during training time, which helps in making the model robust and less likely to rely on any one feature.\n",
    "\n",
    "\n",
    "6)__Complex Classifier Structure:__ AlexNet's classifier is more complex, using three fully connected layers with dropout and ReLU activation functions between them. This is a significant escalation from LeNet’s typically one or two-layer classifiers. This complexity allows AlexNet to make more fine-grained classifications based on the richer feature hierarchies developed in the convolutional layers.\n",
    "\n",
    "\n",
    "7)__Scalability to Higher Resolution Inputs:__ AlexNet is structured to handle higher resolution input images better than LeNet. The initial convolutional layer uses a larger receptive field (11x11 with a stride of 4), which is suitable for larger input images, whereas LeNet is optimized for smaller images typical of earlier datasets like MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359191a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b2fb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cf559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = AlexNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c52cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700dfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f8de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:58<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.6152, Train Accuracy: 21.82%\n",
      "Test Loss: 1.6000, Test Accuracy: 23.17%\n",
      "Updated best model with accuracy: 23.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [01:02<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 1.5954, Train Accuracy: 24.20%\n",
      "Test Loss: 1.5791, Test Accuracy: 22.22%\n",
      "Updated best model with accuracy: 22.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 24/112 [00:13<00:47,  1.84it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the lists to store train and test loss for each epoch\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "best_loss = torch.inf\n",
    "patience = 5\n",
    "epochs_since_best = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]  # For models that return auxiliary outputs\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)  # Store the train loss for this epoch\n",
    "    train_accuracy = 100. * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # For models that return auxiliary outputs\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)  # Store the test loss for this epoch\n",
    "    test_accuracy = 100. * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    # Check for best accuracy and stop if not improved after five more epochs\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        epochs_since_best = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the model\n",
    "        print(f'Updated best model with accuracy: {test_accuracy:.2f}%')\n",
    "    else:\n",
    "        epochs_since_best += 1\n",
    "        if epochs_since_best > patience:\n",
    "            print(\"Stopping early: no improvement after five consecutive epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50172230",
   "metadata": {},
   "source": [
    "### Plot: Epochs vs Train Loss and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34992f93-3258-43b6-88d1-f841cd3ffb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01343dd5-10b6-4197-ae62-bc283bebf0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd538d-2f29-4cd4-8487-a7b13e96d08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f91a51-90dc-4e05-af60-9f3f944a730b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5190427,
     "sourceId": 8662681,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
