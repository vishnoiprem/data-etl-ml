{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8662681,"sourceType":"datasetVersion","datasetId":5190427},{"sourceId":8663090,"sourceType":"datasetVersion","datasetId":5190742}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In the earlier video, we used a pre-trained model VGG16 and no layers were frozen. We trained all the layers that is the feature extraction and classification layeres in the model and updated their weights. In this video let me show you how we can  freeze both feature extractor and earlier classifier layers and only train the last classifier layer. To do so simply set requires_grad to False for all parameters. Then, it explicitly ensures only the parameters of the last classifier layer are trainable. his is done to fine-tune the model only on the new classification task, while keeping the pre-trained weights of the earlier layers fixed. It is recommended to use this approach if you want to leverage the pre-trained VGG16 model as a fixed feature extractor and only train the final classification layer. This is typically faster and requires less data and computational resources compared to fine-tuning the entire model. Let's look at the code block where the change has to be made.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-06-14T06:37:18.194453Z","iopub.status.idle":"2024-06-14T06:37:18.194763Z","shell.execute_reply.started":"2024-06-14T06:37:18.194600Z","shell.execute_reply":"2024-06-14T06:37:18.194613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\n# Define a common image size for VGG16\nimage_size = 224\n\n# Augmentation and transformation for training\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(image_size),  # Crop the given image to random size and aspect ratio.\n    transforms.RandomHorizontalFlip(),         # Horizontally flip the image randomly with a given probability.\n    transforms.RandomRotation(15),             # Random rotation of the image by 15 degrees.\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly change the brightness, contrast, and saturation of an image.\n    transforms.ToTensor(),                    # Convert the image to tensor.\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image with mean and standard deviation.\n])\n\n# Standard transformation for test\ntest_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),  # Resize all images to 299x299 pixels.\n    transforms.CenterCrop(image_size),            # Crop the center of the image to make a square of 299x299.\n    transforms.ToTensor(),                        # Convert the image to tensor.\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image with mean and standard deviation.\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.195931Z","iopub.status.idle":"2024-06-14T06:37:18.196237Z","shell.execute_reply.started":"2024-06-14T06:37:18.196084Z","shell.execute_reply":"2024-06-14T06:37:18.196097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths to the data\ntrain_data_path = r'C:\\Users\\Praphul\\Downloads\\Human Action Recognition_New/train'\ntest_data_path = r'C:\\Users\\Praphul\\Downloads\\Human Action Recognition_New/test'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply the transforms to the train and test data\ntrain_data = datasets.ImageFolder(root=train_data_path, transform=train_transforms)\ntest_data = datasets.ImageFolder(root=test_data_path, transform=test_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.199489Z","iopub.status.idle":"2024-06-14T06:37:18.199839Z","shell.execute_reply.started":"2024-06-14T06:37:18.199653Z","shell.execute_reply":"2024-06-14T06:37:18.199667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataLoader for training and test sets\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.201124Z","iopub.status.idle":"2024-06-14T06:37:18.201424Z","shell.execute_reply.started":"2024-06-14T06:37:18.201270Z","shell.execute_reply":"2024-06-14T06:37:18.201283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run all cells till here.","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Freeze all the feature extractor layers and the earlier classifier layers\nfor param in vgg16.parameters():\n    param.requires_grad = False\n\n# Modify the classifier part of the model to match the number of classes (5 classes)\nvgg16.classifier[6] = nn.Linear(4096, 5)\n\n# Move the model to the device\nmodel = vgg16.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.202600Z","iopub.status.idle":"2024-06-14T06:37:18.202947Z","shell.execute_reply.started":"2024-06-14T06:37:18.202763Z","shell.execute_reply":"2024-06-14T06:37:18.202777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the loss function and the optimizer (only train the last classification layer)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vgg16.classifier[6].parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.204591Z","iopub.status.idle":"2024-06-14T06:37:18.204943Z","shell.execute_reply.started":"2024-06-14T06:37:18.204760Z","shell.execute_reply":"2024-06-14T06:37:18.204773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# Initialize the lists to store train and test loss for each epoch\ntrain_losses = []\ntest_losses = []\n\n# Train the model\nnum_epochs = 20\nbest_loss = torch.inf\npatience = 5\nepochs_since_best = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        if isinstance(outputs, tuple):\n            outputs = outputs[0]  # For models that return auxiliary outputs\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    train_loss = running_loss / len(train_loader)\n    train_losses.append(train_loss)  # Store the train loss for this epoch\n    train_accuracy = 100. * correct / total\n\n    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n\n    # Evaluate on the test set\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]  # For models that return auxiliary outputs\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    test_loss /= len(test_loader)\n    test_losses.append(test_loss)  # Store the test loss for this epoch\n    test_accuracy = 100. * correct / total\n\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n\n    # Check for best accuracy and stop if not improved after five more epochs\n    if test_loss < best_loss:\n        best_loss = test_loss\n        epochs_since_best = 0\n        torch.save(model.state_dict(), 'best_model.pth')  # Save the model\n        print(f'Updated best model with accuracy: {test_accuracy:.2f}%')\n    else:\n        epochs_since_best += 1\n        if epochs_since_best > patience:\n            print(\"Stopping early: no improvement after five consecutive epochs.\")\n            break","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.207243Z","iopub.status.idle":"2024-06-14T06:37:18.207569Z","shell.execute_reply.started":"2024-06-14T06:37:18.207409Z","shell.execute_reply":"2024-06-14T06:37:18.207423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model for evaluation\nvgg16.load_state_dict(torch.load('best_model.pth'))\nvgg16.eval()\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = vgg16(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the best model on the test images: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:18.208460Z","iopub.status.idle":"2024-06-14T06:37:18.208777Z","shell.execute_reply.started":"2024-06-14T06:37:18.208619Z","shell.execute_reply":"2024-06-14T06:37:18.208632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the epochs vs training and test losses\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:37:17.864906Z","iopub.execute_input":"2024-06-14T06:37:17.865524Z","iopub.status.idle":"2024-06-14T06:37:18.193847Z","shell.execute_reply.started":"2024-06-14T06:37:17.865495Z","shell.execute_reply":"2024-06-14T06:37:18.191674Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting the epochs vs training and test losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}